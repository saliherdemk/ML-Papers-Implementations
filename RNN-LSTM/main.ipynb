{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "639cc48e-3f0b-4c10-87e1-4ae0c6e58fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "f = open(\"./TinyShakespeare/input.txt\")\n",
    "text = f.read()\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "89369042-5ef6-4172-8f23-d50f57189ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "95ba4065-0197-4785-a756-d608575ce6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = {ch:i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "def encode(text):\n",
    "    return [char_to_idx[ch] for ch in text]\n",
    "\n",
    "def decode(idx):\n",
    "    return [idx_to_char[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cc8930c4-e038-4c02-b55e-34f74cdc05cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1115394])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "092a8d5e-824d-42e8-9438-6cec7ec9602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d67141f9-7086-40c3-8a16-18c44537f315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18]) tensor(47)\n",
      "tensor([18, 47]) tensor(56)\n",
      "tensor([18, 47, 56]) tensor(57)\n",
      "tensor([18, 47, 56, 57]) tensor(58)\n",
      "tensor([18, 47, 56, 57, 58]) tensor(1)\n",
      "tensor([18, 47, 56, 57, 58,  1]) tensor(15)\n",
      "tensor([18, 47, 56, 57, 58,  1, 15]) tensor(47)\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47]) tensor(58)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length = 8\n",
    "x = train_data[:sequence_length]\n",
    "y = train_data[1:sequence_length + 1]\n",
    "for t in range(sequence_length):\n",
    "    context = x[:t + 1]\n",
    "    target = y[t]\n",
    "    print(context, target)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4fb55732-7703-4444-9d17-1f9a8899c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "class ContextTargetDataset(Dataset):\n",
    "    def __init__(self, data, window_size):\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.window_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = random.randint(0, len(self.data) - self.window_size)\n",
    "        x = self.data[start_idx : start_idx + self.window_size]\n",
    "        y = self.data[start_idx + 1 : start_idx + self.window_size + 1]\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        xs, ys = zip(*batch)\n",
    "        return torch.stack(xs), torch.stack(ys)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "65f3ab6e-c410-411d-af56-a2439f780e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  tensor([[60, 43,  1, 40, 43, 43, 52,  1],\n",
      "        [41, 53, 59, 56, 58,  1, 51, 63]])\n",
      "Y:  tensor([[43,  1, 40, 43, 43, 52,  1, 57],\n",
      "        [53, 59, 56, 58,  1, 51, 63,  1]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataset = ContextTargetDataset(train_data, window_size=8)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2)\n",
    "\n",
    "test_dataset = ContextTargetDataset(test_data, window_size=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2)\n",
    "for batch_x, batch_y in test_loader:\n",
    "    print(\"X: \", batch_x)\n",
    "    print(\"Y: \", batch_y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc143ed-6268-4b6e-bbec-5d20c88b0ee8",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0094c999-df05-48b6-8129-d5afa0ba39da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def sequences_to_dicts(sequences):\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    \n",
    "    all_words = flatten(sequences)\n",
    "    \n",
    "    word_count = defaultdict(int)\n",
    "    for word in flatten(sequences):\n",
    "        word_count[word] += 1\n",
    "\n",
    "    word_count = sorted(list(word_count.items()), key=lambda l: -l[1])\n",
    "\n",
    "    unique_words = [item[0] for item in word_count]\n",
    "    \n",
    "    unique_words.append('UNK')\n",
    "\n",
    "    num_sentences, vocab_size = len(sequences), len(unique_words)\n",
    "\n",
    "    word_to_idx = defaultdict(lambda: vocab_size-1)\n",
    "    idx_to_word = defaultdict(lambda: 'UNK')\n",
    "\n",
    "    for idx, word in enumerate(unique_words):\n",
    "        word_to_idx[word] = idx\n",
    "        idx_to_word[idx] = word\n",
    "\n",
    "    return word_to_idx, idx_to_word, num_sentences, vocab_size\n",
    "\n",
    "\n",
    "word_to_idx, idx_to_word, num_sequences, vocab_size = sequences_to_dicts(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e77f8e54-66c5-498e-ad20-f3b36721543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.inputs[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "def create_datasets(sequences, dataset_class, p_train=0.8):\n",
    "    num_train = int(len(sequences)*p_train)\n",
    "    num_test = int(len(sequences)*(1 - p_train))\n",
    "\n",
    "    sequences_train = sequences[:num_train]\n",
    "    sequences_test = sequences[-num_test:]\n",
    "\n",
    "    def get_inputs_targets_from_sequences(sequences):\n",
    "        inputs, targets = [], []\n",
    "        \n",
    "        for sequence in sequences[2:]:\n",
    "            inputs.append(sequence[:-1])\n",
    "            targets.append(sequence[1:])\n",
    "            \n",
    "        return inputs, targets\n",
    "\n",
    "    inputs_train, targets_train = get_inputs_targets_from_sequences(sequences_train)\n",
    "    inputs_test, targets_test = get_inputs_targets_from_sequences(sequences_test)\n",
    "\n",
    "    training_set = dataset_class(inputs_train, targets_train)\n",
    "    test_set = dataset_class(inputs_test, targets_test)\n",
    "\n",
    "    return training_set, test_set\n",
    "    \n",
    "\n",
    "training_set, test_set = create_datasets(sequences, Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95d4aee6-997a-41ce-b66c-2990fe534332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]],\n",
       "\n",
       "       [[0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot_encode(idx, vocab_size):\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    one_hot[idx] = 1.0\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def one_hot_encode_sequence(sequence, vocab_size):\n",
    "    encoding = np.array([one_hot_encode(word_to_idx[word], vocab_size) for word in sequence])\n",
    "\n",
    "    encoding = encoding.reshape(encoding.shape[0], encoding.shape[1], 1)\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "one_hot_encode_sequence(['a', 'b'], vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e4ebe6f-f682-4765-b4cc-eb825d5d900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "input_size = vocab_size\n",
    "hidden_size = 64\n",
    "output_size = vocab_size\n",
    "\n",
    "def initialize_params():\n",
    "    W_xh = torch.nn.Parameter(torch.randn(hidden_size, input_size) * 0.01)\n",
    "    W_hh = torch.nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.01)\n",
    "    b_h  = torch.nn.Parameter(torch.zeros(hidden_size))\n",
    "    \n",
    "    W_hy = torch.nn.Parameter(torch.randn(output_size, hidden_size) * 0.01)\n",
    "    b_y  = torch.nn.Parameter(torch.zeros(output_size))\n",
    "    \n",
    "    return W_xh, W_hh, b_h, W_hy, b_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e30e6a33-334f-4a84-86ba-9696c1719e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(inputs, W_xh, W_hh, W_hy, b_h, b_y, hidden_size):\n",
    "    h_t = torch.zeros(hidden_size, dtype=torch.float32)\n",
    "    \n",
    "    outputs = []\n",
    "\n",
    "    for x in inputs:\n",
    "        # x = x.squeeze(-1)\n",
    "        h_t = torch.tanh(x @ W_xh + h_t @ W_hh + b_h).to(torch.float32)\n",
    "        y = W_hy @ h_t + b_y\n",
    "        outputs.append(y)\n",
    "\n",
    "    return outputs, h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8f352d0-180e-4ce5-bb94-c3bba8b544ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4x1 and 64x4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tensor(one_hot_encode_sequence(input_tokens, vocab_size), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     16\u001b[0m targets \u001b[38;5;241m=\u001b[39m tensor([word_to_idx[tok] \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m target_tokens])\n\u001b[0;32m---> 18\u001b[0m outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrnn_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_xh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_hh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_hy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(outputs)\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, targets)\n",
      "Cell \u001b[0;32mIn[31], line 8\u001b[0m, in \u001b[0;36mrnn_forward\u001b[0;34m(inputs, W_xh, W_hh, W_hy, b_h, b_y, hidden_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# x = x.squeeze(-1)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     h_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW_xh\u001b[49m \u001b[38;5;241m+\u001b[39m h_t \u001b[38;5;241m@\u001b[39m W_hh \u001b[38;5;241m+\u001b[39m b_h)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      9\u001b[0m     y \u001b[38;5;241m=\u001b[39m W_hy \u001b[38;5;241m@\u001b[39m h_t \u001b[38;5;241m+\u001b[39m b_y\n\u001b[1;32m     10\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(y)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x1 and 64x4)"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import tensor\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "W_xh, W_hh, b_h, W_hy, b_y = initialize_params()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD([W_xh, W_hh, b_h, W_hy, b_y], lr=0.1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for input_tokens, target_tokens in training_set:\n",
    "        inputs = tensor(one_hot_encode_sequence(input_tokens, vocab_size), dtype=torch.float32)\n",
    "        targets = tensor([word_to_idx[tok] for tok in target_tokens])\n",
    "\n",
    "        outputs, _ = rnn_forward(inputs, W_xh, W_hh, W_hy, b_h, b_y, hidden_size)\n",
    "        logits = torch.stack(outputs)\n",
    "\n",
    "        loss = criterion(logits, targets)\n",
    "       \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf217f-f741-4b39-9f74-c55f748f99bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b48595-61df-4ca7-89aa-b650dca23b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d337eae1-c067-4e6f-af02-7179d4f802b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
