# Value-Based vs Policy-Based Methods

### Value-Based Methods

Core idea: Learn a value function and derive a policy from it.

Typical target:

$$
Q_\theta(s, a) \approx Q^*(s, a)
$$

Policy is implicit:

$$
\pi(s) = \arg\max_a Q_\theta(s, a)
$$

For example in [DQN](https://github.com/saliherdemk/snake-rl-dqn/tree/master), we store (state, action, reward, nextState) transitions in a replay buffer and sample them randomly during training. For each sample, we pass the current state through the main network to get Q-values for all actions, which tell us how good each action is. We then pass the next state through a separate target network to estimate the best possible future value from that state. Using the immediate reward and this estimated future return, we compute a target Q-value. Finally, we train the main network so that the Q-value of the action we actually took moves closer to this target. 

```
for (const { state, action, reward, nextState, done } of batch) {
    const currentQ = this.model.forward(state);

    const nextQ = this.targetModel.forward(nextState);
    const targetQ =
        reward + (done ? 0 : this.gamma * Math.max(...nextQ));

    const clippedTargetQ = Math.max(-10, Math.min(10, targetQ));

    this.model.train(currentQ, clippedTargetQ, action);
}
```

### Policy-Based Methods

Core idea: Learn the policy directly, instead of learning a value function and deriving a policy from it.

The policy is explicitly parameterized:

$$
\pi_{\theta}(a | s)
$$

It outputs a probability distribution over actions or parameters of a distribution for continuous actions).

Objective: Instead of approximating $Q^*$ , policy-based methods directly maximize expected return:

$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \right]
$$

The goal is to adjust $\theta$ so that actions leading to higher long-term reward become more likely.

- $\theta$: The parameters (weights) of the policy network. Changing $\theta$ changes how the agent behaves.
- $\pi_{\theta}(a | s)$: The policy parameterized by $\theta$. It defines the probability of taking action a in state s.
- $\tau \sim \pi_{\theta}$: Indicates that trajectories are generated by following the policy $\pi_{\theta}$  in the environment.

$$
\tau = (s_0,a_0,r_0,s_1,a_1,r_1,...)
$$

- $\mathbb{E}[.]$: Expectation over all possible trajectories produced by the policy. This accounts for stochasticity in both the policy and the environment.
- $r_t$: The reward received at timestep t.
- $\gamma \in (0,1]$: The discount factor. Controls how much future rewards are valued relative to immediate rewards.
- $\gamma^t$: Applies exponential discounting to rewards further in the future.
- $J(\theta)$: The expected discounted return of the policy. This is the objective function that policy-based methods aim to maximize.

### PPO

PPO uses two models:

- Actor: policy $\pi_{\theta}(a|s)$: Outputs a probability distribution over actions.
- Critic: value function $V_{\phi}(s)$: Estimates how good a state is in terms of expected future reward.

The critic is used to compute the advantage:

$$
A_t = G_t - V_{\phi}(s_t)
$$

$$
G_t = \sum_{k=0}^{\infty}\gamma^kr_{t+k}
$$

$$
V_{\phi}(s_t) \approx \mathbb{E}[G_t | s_t]
$$








## Resources

- https://medium.com/@eyyu/coding-ppo-from-scratch-with-pytorch-part-2-4-f9d8b8aa938a
- https://www.youtube.com/watch?v=5VHLd9eCZ-wÂ 
