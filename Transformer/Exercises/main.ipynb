{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67aeba07-8e1f-4583-a53f-c68e5fff047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_dates(n=10000):\n",
    "    max_days = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "    dataset = set()\n",
    "    months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "    for _ in range(n):\n",
    "        y = random.randint(1000, 2025)\n",
    "        m = random.randint(1, 12)\n",
    "\n",
    "        if (y % 4 == 0 and y % 100 != 0) or (y % 400 == 0):\n",
    "            max_days[1] = 29\n",
    "        else:\n",
    "            max_days[1] = 28\n",
    "\n",
    "        d = random.randint(1, max_days[m - 1])\n",
    "        dataset.add((f\"{y:04d}-{m:02d}-{d:02d}\", f\"{months[m - 1]} {d}, {y}\"))\n",
    "\n",
    "    return list(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76680890-deb1-444d-82fe-0bd22f37890a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1032-02-20</td>\n",
       "      <td>February 20, 1032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1262-10-21</td>\n",
       "      <td>October 21, 1262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1644-12-20</td>\n",
       "      <td>December 20, 1644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1216-08-31</td>\n",
       "      <td>August 31, 1216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1859-09-25</td>\n",
       "      <td>September 25, 1859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9875</th>\n",
       "      <td>1065-06-17</td>\n",
       "      <td>June 17, 1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9876</th>\n",
       "      <td>1289-02-01</td>\n",
       "      <td>February 1, 1289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9877</th>\n",
       "      <td>1100-03-20</td>\n",
       "      <td>March 20, 1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9878</th>\n",
       "      <td>1373-11-24</td>\n",
       "      <td>November 24, 1373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9879</th>\n",
       "      <td>1329-10-22</td>\n",
       "      <td>October 22, 1329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9880 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               x                   y\n",
       "0     1032-02-20   February 20, 1032\n",
       "1     1262-10-21    October 21, 1262\n",
       "2     1644-12-20   December 20, 1644\n",
       "3     1216-08-31     August 31, 1216\n",
       "4     1859-09-25  September 25, 1859\n",
       "...          ...                 ...\n",
       "9875  1065-06-17       June 17, 1065\n",
       "9876  1289-02-01    February 1, 1289\n",
       "9877  1100-03-20      March 20, 1100\n",
       "9878  1373-11-24   November 24, 1373\n",
       "9879  1329-10-22    October 22, 1329\n",
       "\n",
       "[9880 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = generate_random_dates()\n",
    "df = pd.DataFrame(dataset, columns=[\"x\", \"y\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70ac2e2-fe34-43b7-8d9f-33033b5155ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(18)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"y\"].apply(lambda r : len(r)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1b60dec-aee1-4416-aa01-8587b77a78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        nums = [str(i) for i in range(10)]\n",
    "        uppers = [chr(i) for i in range(ord('A'), ord('Z') + 1)]\n",
    "        lowers = [chr(i) for i in range(ord('a'), ord('z') + 1)]\n",
    "        self.input_max = 10 + 2\n",
    "        self.output_max = 18 + 2\n",
    "        self.vocab = nums + uppers + lowers + [\"-\", \",\", \" \", \"<sos>\", \"<eos>\", \"<pad>\"]\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.tokens_to_ids = {str(self.vocab[i]): i for i in range(len(self.vocab))}\n",
    "        self.ids_to_tokens = {str(i): str(self.vocab[i]) for i in range(len(self.vocab))}\n",
    "\n",
    "        self.pad_token_id = self.tokens_to_ids[\"<pad>\"]\n",
    "\n",
    "    def encode(self, sample):\n",
    "        x, y = sample\n",
    "        x = [\"<sos>\"] + list(x) + [\"<eos>\"]\n",
    "        y = [\"<sos>\"] + list(y) + [\"<eos>\"]\n",
    "        while len(x) != self.input_max: x.append(\"<pad>\")\n",
    "        while len(y) != self.output_max: y.append(\"<pad>\")\n",
    "        res_x = [self.tokens_to_ids[i] for i in x]\n",
    "        res_y = [self.tokens_to_ids[i] for i in y]\n",
    "\n",
    "        return (res_x, res_y)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        res = [self.ids_to_tokens[str(i)] for i in ids]\n",
    "        return \"\".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3545a45-7c78-456b-bd24-3777b1580b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<sos>1032-02-20<eos>',\n",
       " '1032-02-20',\n",
       " [65, 1, 0, 3, 2, 62, 0, 2, 62, 2, 0, 66],\n",
       " '-----',\n",
       " '<sos>February 20, 1032<eos><pad>',\n",
       " 'February 20, 1032',\n",
       " [65, 15, 40, 37, 53, 56, 36, 53, 60, 64, 2, 0, 63, 64, 1, 0, 3, 2, 66, 67])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "a = dataset[0]\n",
    "encoded_x, encoded_y = tokenizer.encode(a)\n",
    "tokenizer.decode(encoded_x), a[0], encoded_x, \"-----\", tokenizer.decode(encoded_y), a[1], encoded_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67e542d8-86ad-469a-88db-853bf866d4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68,\n",
       " 61,\n",
       " ('1032-02-20', 'February 20, 1032'),\n",
       " [65, 1, 0, 3, 2, 62, 0, 2, 62, 2, 0, 66])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size,tokenizer.tokens_to_ids[\"z\"], a, encoded_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1f87cd1-a978-4b10-afdf-1c36e1c0aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DateDataset(Dataset):\n",
    "    def __init__(self,data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_encoded, y_encoded = self.tokenizer.encode(self.data[idx])\n",
    "        \n",
    "        x_tensor = torch.tensor(x_encoded, dtype=torch.long)\n",
    "        y_tensor = torch.tensor(y_encoded, dtype=torch.long)\n",
    "\n",
    "        return x_tensor, y_tensor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8241cb4d-bbb4-4026-9022-aba128f4e1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[65,  2,  0,  0,  4, 62,  0,  4, 62,  1,  1, 66],\n",
      "        [65,  1,  9,  5,  0, 62,  0,  7, 62,  0,  4, 66],\n",
      "        [65,  1,  6,  7,  3, 62,  1,  2, 62,  0,  9, 66],\n",
      "        [65,  1,  4,  3,  3, 62,  1,  2, 62,  1,  7, 66],\n",
      "        [65,  1,  2,  9,  5, 62,  0,  3, 62,  2,  5, 66]])\n",
      "tensor([[65, 10, 51, 53, 44, 47, 64,  1,  1, 63, 64,  2,  0,  0,  4, 66, 67, 67,\n",
      "         67, 67],\n",
      "        [65, 19, 56, 47, 60, 64,  4, 63, 64,  1,  9,  5,  0, 66, 67, 67, 67, 67,\n",
      "         67, 67],\n",
      "        [65, 13, 40, 38, 40, 48, 37, 40, 53, 64,  9, 63, 64,  1,  6,  7,  3, 66,\n",
      "         67, 67],\n",
      "        [65, 13, 40, 38, 40, 48, 37, 40, 53, 64,  1,  7, 63, 64,  1,  4,  3,  3,\n",
      "         66, 67],\n",
      "        [65, 22, 36, 53, 38, 43, 64,  2,  5, 63, 64,  1,  2,  9,  5, 66, 67, 67,\n",
      "         67, 67]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "date_dataset = DateDataset(dataset, tokenizer)\n",
    "dataloader = DataLoader(date_dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "for batch_idx, (x_batch, y_batch) in enumerate(dataloader):\n",
    "    print(x_batch)\n",
    "    print(y_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4623a119-d819-48f5-82bc-b7df7a010d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        return self.embedding(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3e4fabd-7e7a-4b58-b2cc-9a66dbaa198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len, n = 10000):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        positions = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(self.n) / embed_dim))\n",
    "                \n",
    "        pe[:, 0::2] = torch.sin(positions * div_term)\n",
    "        pe[:, 1::2] = torch.cos(positions * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        pe = self.pe[:seq_len, :]\n",
    "        pe = pe.unsqueeze(0)\n",
    "        pe = pe.expand(x.size(0), -1, -1)\n",
    "        # print(x[0], pe[0])\n",
    "        return x + pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2998f329-e1e1-4f0a-828b-7608f7650067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class EmbeddingBlock(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_len, pad_id, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = PositionalEncoder(embed_dim, max_len)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        tok_emb = self.token_emb(token_ids) * math.sqrt(self.embed_dim)\n",
    "        # print(\"te\",tok_emb)\n",
    "        x = self.pos_emb(tok_emb)\n",
    "        # print(\"pe\", x)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "27b51e4a-760c-4f5d-ba01-80b521a07f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy = torch.tensor([[65, 0, 62, 65]])\n",
    "src_embedding_block = EmbeddingBlock(tokenizer.vocab_size, embed_dim, max_seq_len, pad_id=tokenizer.pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "84e4e33d-1224-42ce-b2af-89cc1b3b3f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te tensor([[[-1.0169,  3.7117, -3.0031,  0.0366, -2.0964, -4.5249, -5.8190,\n",
      "           0.1423,  6.3528, -5.9342, -2.7633, -6.5703, -1.8161, -1.1127,\n",
      "          -3.3698,  1.6234],\n",
      "         [ 2.5321, -0.1290, -0.9900, -3.7510,  4.6948,  0.8859, -1.5571,\n",
      "          -2.4139, -8.1228,  1.2832, -0.7425,  1.6549, -2.8551,  2.4007,\n",
      "           5.5863,  6.4642],\n",
      "         [-1.2332, -1.0142, -4.4233,  1.6503,  0.4995,  2.1766, -4.7730,\n",
      "          -1.4999,  3.7029,  0.3568,  4.4813, -2.4508, -3.3141,  8.0293,\n",
      "          -2.5632, -2.9621],\n",
      "         [-1.0169,  3.7117, -3.0031,  0.0366, -2.0964, -4.5249, -5.8190,\n",
      "           0.1423,  6.3528, -5.9342, -2.7633, -6.5703, -1.8161, -1.1127,\n",
      "          -3.3698,  1.6234]]], grad_fn=<MulBackward0>)\n",
      "pe tensor([[[-1.0169,  4.7117, -3.0031,  1.0366, -2.0964, -3.5249, -5.8190,\n",
      "           1.1423,  6.3528, -4.9342, -2.7633, -5.5703, -1.8161, -0.1127,\n",
      "          -3.3698,  2.6234],\n",
      "         [ 3.3736,  0.4113, -0.6790, -2.8006,  4.7947,  1.8809, -1.5255,\n",
      "          -1.4144, -8.1128,  2.2831, -0.7393,  2.6549, -2.8541,  3.4007,\n",
      "           5.5866,  7.4642],\n",
      "         [-0.3239, -1.4303, -3.8322,  2.4569,  0.6982,  3.1566, -4.7098,\n",
      "          -0.5019,  3.7229,  1.3566,  4.4877, -1.4508, -3.3121,  9.0293,\n",
      "          -2.5626, -1.9621],\n",
      "         [-0.8758,  2.7217, -2.1904,  0.6194, -1.8009, -3.5696, -5.7243,\n",
      "           1.1378,  6.3828, -4.9347, -2.7538, -5.5704, -1.8131, -0.1127,\n",
      "          -3.3689,  2.6234]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "src_emb = src_embedding_block(toy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da373a07-8a19-4c70-b8d2-fdf22ca05680",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 16\n",
    "max_seq_len = 20\n",
    "\n",
    "src_embedding_block = EmbeddingBlock(tokenizer.vocab_size, embed_dim, max_seq_len, pad_id=tokenizer.pad_token_id)\n",
    "tgt_embedding_block = EmbeddingBlock(tokenizer.vocab_size, embed_dim, max_seq_len, pad_id=tokenizer.pad_token_id)\n",
    "\n",
    "for batch_idx, (x_batch, y_batch) in enumerate(dataloader[:5]):\n",
    "    src_emb = src_embedding_block(x_batch)\n",
    "    tgt_emb = tgt_embedding_block(y_batch)\n",
    "    print(src_emb.shape, tgt_emb.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b222c4fe-7e67-48f5-ac11-46aca6c64f6c",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a5b0648-d566-4dc5-b4c5-349c02a0fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = generate_random_dates()\n",
    "df = pd.DataFrame(dataset, columns=[\"x\", \"y\"])\n",
    "\n",
    "date_dataset = DateDataset(dataset, tokenizer)\n",
    "dataloader = DataLoader(date_dataset, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0aa71756-d219-4db0-a0eb-6b8fd7051bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 12, 16]) torch.Size([5, 20, 16])\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 16\n",
    "max_seq_len = 20\n",
    "\n",
    "src_embedding_block = EmbeddingBlock(tokenizer.vocab_size, embed_dim, max_seq_len, pad_id=tokenizer.pad_token_id)\n",
    "tgt_embedding_block = EmbeddingBlock(tokenizer.vocab_size, embed_dim, max_seq_len, pad_id=tokenizer.pad_token_id)\n",
    "\n",
    "for batch_idx, (x_batch, y_batch) in enumerate(dataloader):\n",
    "    src_emb = src_embedding_block(x_batch)\n",
    "    tgt_emb = tgt_embedding_block(y_batch)\n",
    "    print(src_emb.shape, tgt_emb.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7d56ebe-a939-4af5-af47-73fb0dfd171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x[0])\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        print(Q[0])\n",
    "        print(K.transpose(-2, -1)[0])\n",
    "        \n",
    "\n",
    "        scores = Q @ K.transpose(-2, -1) # QK^T / sqrt(d_k)\n",
    "\n",
    "        # print(scores[0])\n",
    "        scores = F.softmax(Q @ K.transpose(-2, -1) / (self.embed_dim ** 0.5)) # QK^T / sqrt(d_k)\n",
    "        print(scores[0])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b51c50b1-f391-41c4-afd3-5eab2d173ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6077, -1.1013,  2.7406, -0.7540,  0.3023,  2.9211,  2.6975,  1.8287,\n",
      "         -0.9295, -3.0172,  3.9800, -0.2289, -1.1133,  3.7010, -1.7028, -4.0110],\n",
      "        [ 3.2174, -1.6857,  1.7360,  0.3697,  0.4990,  1.3540,  1.5095, -3.2122,\n",
      "          1.8186,  0.1438,  0.2113,  0.7156,  1.5320,  3.8769,  0.6799, -3.2036],\n",
      "        [ 1.1522,  0.8753, -0.4475,  2.6429,  1.7090, -0.9089,  1.1222,  1.4286,\n",
      "          0.2103,  3.7216, -1.0529, -0.7472, -5.0433, -1.0415,  5.4696,  3.8084],\n",
      "        [-2.2415,  0.5077, -5.3882,  2.1771, -2.9984, -0.9999, -1.8916,  0.7201,\n",
      "         -1.6512,  2.2896, -1.5486,  0.1555,  2.3028, -2.5760, -3.7443,  0.6497],\n",
      "        [ 0.1556,  0.3669, -1.1607,  3.9464,  0.2232,  0.4521,  0.1981,  1.3263,\n",
      "         -2.4108,  3.6452,  0.7514, -0.3106, -2.8094, -1.3884,  2.5923,  1.2330],\n",
      "        [ 0.6471, -2.3877, -3.4905, -4.1567,  0.8856,  2.9045,  1.3073, -4.2904,\n",
      "         -2.8911,  0.8365, -2.4124, -0.9344, -5.5669, -0.1383, -5.1528,  3.6730],\n",
      "        [ 0.7403, -1.9670,  1.5687,  4.3700, -1.0816, -2.4459, -2.6272,  0.4576,\n",
      "         -1.3372, -1.3382, -0.6174,  4.1250,  2.5285,  1.6595,  2.1491, -0.4335],\n",
      "        [-1.1969,  0.4425,  1.3011,  5.7884,  1.2613, -0.5143, -0.3780,  2.1631,\n",
      "         -4.6633,  0.5088,  2.9163,  1.8813, -1.9383,  1.4725,  1.4267,  0.3837],\n",
      "        [ 1.4377, -2.5596, -0.3819, -2.3122,  1.6183,  4.8464,  1.5753, -2.2372,\n",
      "         -3.5239, -0.2703, -0.6740, -0.1057, -6.4521,  4.2740, -3.6226,  2.9917],\n",
      "        [ 1.9335, -0.1978,  0.3868, -1.1029, -0.1160,  0.4619,  3.6150, -5.3302,\n",
      "          3.5691, -1.3705, -1.0422,  1.5496, -0.4679,  1.8795,  0.0892, -1.5529],\n",
      "        [-1.4574, -1.8209,  1.9580,  3.4249, -0.5886, -1.0020, -3.6318,  3.3595,\n",
      "         -0.9340,  0.4663,  2.7928,  2.3823, -2.7050,  3.9364, -2.3472,  0.6238],\n",
      "        [ 0.9101,  0.3599, -0.5490, -3.4814,  3.5535,  1.6540, -0.2471, -0.3989,\n",
      "         -2.1880, -1.2757, -0.9009, -0.0520, -0.9470,  0.3095, -0.7589,  2.7097]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[6.9168e-01, 2.2506e-05, 1.3485e-05, 4.0858e-04, 1.7615e-04, 1.6100e-04,\n",
      "         2.2532e-03, 1.3781e-02, 6.8604e-05, 2.7220e-05, 1.7865e-01, 4.3361e-01],\n",
      "        [1.4713e-02, 8.4773e-02, 6.0617e-05, 4.2632e-09, 5.1651e-01, 8.3995e-04,\n",
      "         4.9056e-04, 6.5711e-02, 3.8125e-02, 3.5976e-03, 1.8383e-05, 3.3456e-02],\n",
      "        [6.3600e-01, 2.5396e-05, 8.6047e-02, 1.1421e-09, 2.4128e-03, 7.7224e-03,\n",
      "         1.9693e-08, 4.3319e-01, 9.2110e-01, 7.4588e-01, 6.3564e-06, 5.2529e-03],\n",
      "        [9.9140e-09, 1.6042e-04, 1.8209e-04, 1.6806e-02, 3.0163e-07, 3.6217e-08,\n",
      "         1.7739e-07, 7.4129e-07, 2.7266e-10, 1.4982e-06, 4.8811e-07, 5.2713e-04],\n",
      "        [1.0502e-03, 9.5997e-01, 1.3004e-02, 8.8615e-08, 5.2586e-07, 4.8700e-03,\n",
      "         9.6210e-07, 5.5731e-03, 1.3358e-03, 9.9909e-01, 1.5075e-04, 2.6343e-05],\n",
      "        [1.9233e-04, 1.0184e-01, 2.8512e-03, 5.1917e-01, 3.2190e-07, 1.1768e-07,\n",
      "         2.2625e-03, 1.0004e-02, 6.5937e-08, 7.6174e-05, 2.5045e-02, 4.3104e-03],\n",
      "        [4.5525e-01, 2.2267e-01, 3.1530e-01, 2.1127e-04, 1.6843e-03, 2.4598e-01,\n",
      "         5.1052e-02, 7.9541e-04, 6.3241e-02, 1.0624e-03, 7.4088e-03, 8.1253e-02],\n",
      "        [1.2070e-03, 5.2359e-05, 3.0028e-02, 9.3714e-01, 1.3462e-06, 5.4057e-02,\n",
      "         8.9919e-05, 9.0674e-08, 1.3077e-03, 9.0510e-07, 7.0987e-04, 6.4355e-01],\n",
      "        [1.0509e-02, 1.6303e-03, 4.7025e-05, 1.7996e-01, 3.7763e-08, 6.4185e-07,\n",
      "         3.1140e-03, 2.7296e-01, 6.4915e-06, 1.1935e-05, 3.2225e-01, 6.5549e-01],\n",
      "        [1.8732e-03, 5.8103e-02, 2.7690e-01, 2.0440e-11, 9.9998e-01, 3.1412e-06,\n",
      "         6.8803e-06, 3.8965e-01, 1.4795e-02, 9.4491e-03, 1.0194e-04, 5.1763e-05],\n",
      "        [5.3429e-06, 2.3475e-11, 1.8489e-05, 9.9507e-01, 3.9504e-04, 4.4961e-04,\n",
      "         3.1785e-02, 9.6607e-08, 2.9969e-06, 1.5398e-12, 3.9381e-02, 5.7132e-01],\n",
      "        [1.6479e-01, 5.4316e-02, 6.1667e-02, 2.3512e-03, 6.0253e-05, 2.6235e-04,\n",
      "         1.3468e-01, 3.3523e-01, 4.4589e-02, 8.2753e-03, 5.3568e-01, 1.3355e-01]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_149791/2251856165.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  scores = F.softmax(Q @ K.transpose(-2, -1) / (self.embed_dim ** 0.5)) # QK^T / sqrt(d_k)\n"
     ]
    }
   ],
   "source": [
    "att_layer = SelfAttention(embed_dim)\n",
    "\n",
    "x_batch, y_batch = next(iter(dataloader))\n",
    "src_emb = src_embedding_block(x_batch)\n",
    "tgt_emb = tgt_embedding_block(y_batch)\n",
    "att_layer.forward(src_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35df3237-6d05-4cd1-9147-bf150dc8dc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_149791/2335337580.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  qk = F.softmax(tensor / (16 ** 0.5))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 16])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([\n",
    "    [-5.3763, -19.0716, -10.7304, 16.2014, 25.0778, -29.9709, 14.1424, 47.5504, -3.9680, -12.2618, -53.3239, -0.6131],\n",
    "    [28.7324, -9.4999, -6.8611, -18.2616, -27.7456, -1.1073, -23.4516, -56.3575, -16.1110, -13.3637, 59.1139, 0.8883],\n",
    "    [33.2356, -12.3105, -2.6957, -10.5091, -21.9326, -11.1053, -16.4659, -49.8313, -20.0018, -13.4830, 47.4658, -1.2959],\n",
    "    [6.2911, -16.4541, -5.6001, -1.2533, 9.2225, 6.3236, 4.9841, 13.6948, 5.7757, -15.1106, 23.0964, 18.0278],\n",
    "    [0.9326, -1.4808, -2.6664, -6.2392, -4.2478, 13.2531, -0.6133, -20.2357, -6.1465, -2.5461, 49.7770, 9.5165],\n",
    "    [-2.5958, -18.1085, -16.3908, 0.9187, 3.2732, 5.0221, -10.9709, -2.9974, 7.5171, -15.0100, -29.5982, -29.8554],\n",
    "    [16.0027, -18.9854, -7.9330, -8.5611, -2.2667, 11.0441, -6.8223, -4.7966, 8.5525, -19.0153, 32.5196, 7.5894],\n",
    "    [11.6683, 10.5559, 3.9735, -4.1875, -11.2270, 12.0608, -6.8960, -41.2959, -0.8020, 8.1903, 44.3070, -19.1285],\n",
    "    [-1.0612, -17.3225, -17.5310, 8.7398, 22.2118, 6.8019, 1.0091, 27.6504, 18.2031, -12.3405, -25.6904, -16.0338],\n",
    "    [26.6676, -5.4987, -4.8026, -16.8313, -27.4139, 5.4588, -19.6466, -55.2616, -11.5514, -9.9637, 67.6490, 4.1979],\n",
    "    [5.6592, 0.4981, 0.8265, 1.3560, -6.6801, 18.6764, -8.1335, -19.9559, 8.5394, -1.0782, -2.8182, -6.0825],\n",
    "    [11.7751, 3.8447, 12.8273, 3.6451, -13.5613, -25.4471, -6.3279, -36.0715, -15.5435, 4.4850, -15.7695, -45.6263]\n",
    "])\n",
    "\n",
    "V = torch.tensor([[ 0.0523, -0.3169,  1.0038,  3.8029,  2.8152,  0.5570, -2.9845,  1.6947,\n",
    "         -2.7567, -2.1093,  2.7524,  0.4722,  1.6745,  1.0933,  3.1662,  3.1196],\n",
    "        [ 0.3888, -0.2766,  3.2932,  1.1321,  0.7103,  2.4821, -3.0863, -2.4968,\n",
    "         -4.2470,  0.0247, -1.5927,  1.8743, -1.3891,  4.1220, -0.6374, -0.6978],\n",
    "        [ 0.1997, -0.4959,  3.5477,  0.9209,  0.8466,  2.6905, -2.9755, -2.7062,\n",
    "         -4.1371,  0.1154, -1.9207,  1.8367, -1.2775,  4.0197, -0.4618, -0.7433],\n",
    "        [ 4.3301, -1.1908, -1.9410,  0.7849, -0.0856,  2.9224, -0.0149, -1.3127,\n",
    "         -3.1154, -3.8022,  0.1690,  4.6527,  3.1757,  0.1667,  1.6034, -1.5607],\n",
    "        [-1.2611, -3.7500,  4.4421,  0.0624,  0.6921,  2.0781,  3.5267,  1.5235,\n",
    "         -3.4005, -3.0553, -0.2118,  1.6587, -1.7829,  0.5463,  2.3267,  4.3869],\n",
    "        [-2.1013,  1.1058, -0.4530, -4.4448, -0.0611, -2.5772, -3.5890, -3.7889,\n",
    "         -1.7608,  2.1861,  0.4290, -1.4026,  2.0943, -3.5062,  0.1753, -2.6732],\n",
    "        [ 0.6525, -0.6348,  3.5759,  1.3017, -0.1698,  2.7247, -2.7032, -2.0993,\n",
    "         -4.3713, -0.3325, -2.0410,  2.5782, -1.6670,  4.3540, -0.9295, -1.0574],\n",
    "        [-3.9596, -0.5530,  2.6572,  0.2565, -2.0472, -1.4750, -2.1472, -1.0649,\n",
    "         -3.5263,  1.6453, -2.2381, -0.0380, -1.5689,  0.1930,  2.5177,  0.4268],\n",
    "        [-1.9230,  1.6798, -2.2871, -6.8718,  3.1228, -3.3532, -5.0108, -5.6930,\n",
    "          0.4687,  1.3645,  2.1050, -2.3289,  2.7139, -3.6273, -0.4498, -1.8997],\n",
    "        [-0.2357, -1.2845,  2.3008, -0.4310, -0.5731,  2.0083, -2.0417, -0.6014,\n",
    "         -2.4623, -2.5855, -1.1960,  2.5856, -0.0853,  2.5082,  0.3033, -0.8056],\n",
    "        [ 5.9674, -3.6949, -3.9467, -2.4978,  1.6004,  1.8320, -0.7655, -2.8905,\n",
    "          2.5294, -1.2686, -0.9696,  0.9364,  6.0051, -4.1793,  3.2551, -1.7872],\n",
    "        [ 1.8519, -4.3785, -5.7870, -2.8644, -8.4280, -2.5224,  3.6461,  0.2399,\n",
    "         -2.5877, -0.3564,  4.7782, -3.0841, -0.6588, -3.3774, -0.0579,  0.5228]])\n",
    "\n",
    "qk = F.softmax(tensor / (16 ** 0.5))\n",
    "att_output = qk @ V\n",
    "\n",
    "mlp =nn.Sequential(nn.Linear(embed_dim, 16), nn.ReLU(),nn.Linear(16,embed_dim))\n",
    "\n",
    "mlp(att_output).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eacb0e9-4c86-44a5-86bb-011a65426f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EncoderLayer(nn.Module):\n",
    "#     def __init__(self,embed_dim, num_heads, ff_hidden_dim, dropout=0.1):\n",
    "#         super.__init__()\n",
    "#         self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "#         self.norm1 = nn.LayerNorm(embed_dim)\n",
    "#         self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "#         self.ff = nn.Sequential(\n",
    "#             nn.Linear(embed_dim, ff_hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(ff_hidden_dim, embed_dim),\n",
    "#         )\n",
    "#         self.norm2 = nn.LayerNorm(embed_dim)\n",
    "#         self.dropout2 = nn.Dropout(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d110d8ca-f796-4c86-a63f-6d51ad06f1e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
