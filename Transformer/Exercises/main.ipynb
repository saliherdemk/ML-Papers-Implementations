{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67aeba07-8e1f-4583-a53f-c68e5fff047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_dates(n=10000):\n",
    "    max_days = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "    dataset = set()\n",
    "    months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "    for _ in range(n):\n",
    "        y = random.randint(1000, 2025)\n",
    "        m = random.randint(1, 12)\n",
    "\n",
    "        if (y % 4 == 0 and y % 100 != 0) or (y % 400 == 0):\n",
    "            max_days[1] = 29\n",
    "        else:\n",
    "            max_days[1] = 28\n",
    "\n",
    "        d = random.randint(1, max_days[m - 1])\n",
    "        dataset.add((f\"{y:04d}-{m:02d}-{d:02d}\", f\"{months[m - 1]} {d}, {y}\"))\n",
    "\n",
    "    return list(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76680890-deb1-444d-82fe-0bd22f37890a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1676-11-30</td>\n",
       "      <td>November 30, 1676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1162-02-03</td>\n",
       "      <td>February 3, 1162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1394-03-23</td>\n",
       "      <td>March 23, 1394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1616-07-06</td>\n",
       "      <td>July 6, 1616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1647-05-03</td>\n",
       "      <td>May 3, 1647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9849</th>\n",
       "      <td>1577-03-14</td>\n",
       "      <td>March 14, 1577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9850</th>\n",
       "      <td>1763-05-23</td>\n",
       "      <td>May 23, 1763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9851</th>\n",
       "      <td>1998-08-29</td>\n",
       "      <td>August 29, 1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9852</th>\n",
       "      <td>1087-10-21</td>\n",
       "      <td>October 21, 1087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9853</th>\n",
       "      <td>1951-01-06</td>\n",
       "      <td>January 6, 1951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9854 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               x                  y\n",
       "0     1676-11-30  November 30, 1676\n",
       "1     1162-02-03   February 3, 1162\n",
       "2     1394-03-23     March 23, 1394\n",
       "3     1616-07-06       July 6, 1616\n",
       "4     1647-05-03        May 3, 1647\n",
       "...          ...                ...\n",
       "9849  1577-03-14     March 14, 1577\n",
       "9850  1763-05-23       May 23, 1763\n",
       "9851  1998-08-29    August 29, 1998\n",
       "9852  1087-10-21   October 21, 1087\n",
       "9853  1951-01-06    January 6, 1951\n",
       "\n",
       "[9854 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = generate_random_dates()\n",
    "df = pd.DataFrame(dataset, columns=[\"x\", \"y\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70ac2e2-fe34-43b7-8d9f-33033b5155ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(18)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"y\"].apply(lambda r : len(r)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1b60dec-aee1-4416-aa01-8587b77a78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        nums = [str(i) for i in range(10)]\n",
    "        uppers = [chr(i) for i in range(ord('A'), ord('Z') + 1)]\n",
    "        lowers = [chr(i) for i in range(ord('a'), ord('z') + 1)]\n",
    "        self.input_max = 10 + 2\n",
    "        self.output_max = 18 + 2\n",
    "        self.vocab = nums + uppers + lowers + [\"-\", \",\", \" \", \"<sos>\", \"<eos>\", \"<pad>\"]\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.tokens_to_ids = {str(self.vocab[i]): i for i in range(len(self.vocab))}\n",
    "        self.ids_to_tokens = {str(i): str(self.vocab[i]) for i in range(len(self.vocab))}\n",
    "\n",
    "        self.pad_token_id = self.tokens_to_ids[\"<pad>\"]\n",
    "\n",
    "    def encode(self, sample):\n",
    "        x, y = sample\n",
    "        x = [\"<sos>\"] + list(x) + [\"<eos>\"]\n",
    "        y = [\"<sos>\"] + list(y) + [\"<eos>\"]\n",
    "        while len(x) != self.input_max: x.append(\"<pad>\")\n",
    "        while len(y) != self.output_max: y.append(\"<pad>\")\n",
    "        res_x = [self.tokens_to_ids[i] for i in x]\n",
    "        res_y = [self.tokens_to_ids[i] for i in y]\n",
    "\n",
    "        return (res_x, res_y)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        res = [self.ids_to_tokens[str(i)] for i in ids]\n",
    "        return \"\".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d3545a45-7c78-456b-bd24-3777b1580b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<sos>1676-11-30<eos>',\n",
       " '1676-11-30',\n",
       " [65, 1, 6, 7, 6, 62, 1, 1, 62, 3, 0, 66],\n",
       " '-----',\n",
       " '<sos>November 30, 1676<eos><pad>',\n",
       " 'November 30, 1676',\n",
       " [65, 23, 50, 57, 40, 48, 37, 40, 53, 64, 3, 0, 63, 64, 1, 6, 7, 6, 66, 67])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "a = dataset[0]\n",
    "encoded_x, encoded_y = tokenizer.encode(a)\n",
    "tokenizer.decode(encoded_x), a[0], encoded_x, \"-----\", tokenizer.decode(encoded_y), a[1], encoded_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67e542d8-86ad-469a-88db-853bf866d4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68,\n",
       " 61,\n",
       " ('1676-11-30', 'November 30, 1676'),\n",
       " [65, 1, 6, 7, 6, 62, 1, 1, 62, 3, 0, 66])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size,tokenizer.tokens_to_ids[\"z\"], a, encoded_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1f87cd1-a978-4b10-afdf-1c36e1c0aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DateDataset(Dataset):\n",
    "    def __init__(self,data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_encoded, y_encoded = self.tokenizer.encode(self.data[idx])\n",
    "        \n",
    "        x_tensor = torch.tensor(x_encoded, dtype=torch.long)\n",
    "        y_tensor = torch.tensor(y_encoded, dtype=torch.long)\n",
    "\n",
    "        return x_tensor, y_tensor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8241cb4d-bbb4-4026-9022-aba128f4e1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[65,  1,  8,  3,  2, 62,  0,  5, 62,  1,  7, 66],\n",
      "        [65,  1,  7,  8,  4, 62,  1,  0, 62,  0,  2, 66],\n",
      "        [65,  1,  2,  7,  6, 62,  0,  5, 62,  0,  5, 66],\n",
      "        [65,  1,  3,  0,  7, 62,  1,  0, 62,  2,  3, 66],\n",
      "        [65,  1,  9,  2,  2, 62,  0,  5, 62,  1,  5, 66]])\n",
      "tensor([[65, 22, 36, 60, 64,  1,  7, 63, 64,  1,  8,  3,  2, 66, 67, 67, 67, 67,\n",
      "         67, 67],\n",
      "        [65, 24, 38, 55, 50, 37, 40, 53, 64,  2, 63, 64,  1,  7,  8,  4, 66, 67,\n",
      "         67, 67],\n",
      "        [65, 22, 36, 60, 64,  5, 63, 64,  1,  2,  7,  6, 66, 67, 67, 67, 67, 67,\n",
      "         67, 67],\n",
      "        [65, 24, 38, 55, 50, 37, 40, 53, 64,  2,  3, 63, 64,  1,  3,  0,  7, 66,\n",
      "         67, 67],\n",
      "        [65, 22, 36, 60, 64,  1,  5, 63, 64,  1,  9,  2,  2, 66, 67, 67, 67, 67,\n",
      "         67, 67]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "date_dataset = DateDataset(dataset, tokenizer)\n",
    "dataloader = DataLoader(date_dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "for batch_idx, (x_batch, y_batch) in enumerate(dataloader):\n",
    "    print(x_batch)\n",
    "    print(y_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4623a119-d819-48f5-82bc-b7df7a010d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        return self.embedding(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3e4fabd-7e7a-4b58-b2cc-9a66dbaa198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len, n = 10000):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        positions = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(self.n) / embed_dim))\n",
    "                \n",
    "        pe[:, 0::2] = torch.sin(positions * div_term)\n",
    "        pe[:, 1::2] = torch.cos(positions * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        pe = self.pe[:seq_len, :]\n",
    "        pe = pe.unsqueeze(0)\n",
    "        pe = pe.expand(x.size(0), -1, -1)\n",
    "        # print(x[0], pe[0])\n",
    "        return x + pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2998f329-e1e1-4f0a-828b-7608f7650067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class EmbeddingBlock(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_len, pad_id, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = PositionalEncoder(embed_dim, max_len)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        tok_emb = self.token_emb(token_ids) * math.sqrt(self.embed_dim)\n",
    "        # print(\"te\",tok_emb)\n",
    "        x = self.pos_emb(tok_emb)\n",
    "        # print(\"pe\", x)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "27b51e4a-760c-4f5d-ba01-80b521a07f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy = torch.tensor([[65, 0, 62, 65]])\n",
    "src_embedding_block = EmbeddingBlock(tokenizer.vocab_size, embed_dim, max_seq_len, pad_id=tokenizer.pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "84e4e33d-1224-42ce-b2af-89cc1b3b3f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te tensor([[[-1.0169,  3.7117, -3.0031,  0.0366, -2.0964, -4.5249, -5.8190,\n",
      "           0.1423,  6.3528, -5.9342, -2.7633, -6.5703, -1.8161, -1.1127,\n",
      "          -3.3698,  1.6234],\n",
      "         [ 2.5321, -0.1290, -0.9900, -3.7510,  4.6948,  0.8859, -1.5571,\n",
      "          -2.4139, -8.1228,  1.2832, -0.7425,  1.6549, -2.8551,  2.4007,\n",
      "           5.5863,  6.4642],\n",
      "         [-1.2332, -1.0142, -4.4233,  1.6503,  0.4995,  2.1766, -4.7730,\n",
      "          -1.4999,  3.7029,  0.3568,  4.4813, -2.4508, -3.3141,  8.0293,\n",
      "          -2.5632, -2.9621],\n",
      "         [-1.0169,  3.7117, -3.0031,  0.0366, -2.0964, -4.5249, -5.8190,\n",
      "           0.1423,  6.3528, -5.9342, -2.7633, -6.5703, -1.8161, -1.1127,\n",
      "          -3.3698,  1.6234]]], grad_fn=<MulBackward0>)\n",
      "pe tensor([[[-1.0169,  4.7117, -3.0031,  1.0366, -2.0964, -3.5249, -5.8190,\n",
      "           1.1423,  6.3528, -4.9342, -2.7633, -5.5703, -1.8161, -0.1127,\n",
      "          -3.3698,  2.6234],\n",
      "         [ 3.3736,  0.4113, -0.6790, -2.8006,  4.7947,  1.8809, -1.5255,\n",
      "          -1.4144, -8.1128,  2.2831, -0.7393,  2.6549, -2.8541,  3.4007,\n",
      "           5.5866,  7.4642],\n",
      "         [-0.3239, -1.4303, -3.8322,  2.4569,  0.6982,  3.1566, -4.7098,\n",
      "          -0.5019,  3.7229,  1.3566,  4.4877, -1.4508, -3.3121,  9.0293,\n",
      "          -2.5626, -1.9621],\n",
      "         [-0.8758,  2.7217, -2.1904,  0.6194, -1.8009, -3.5696, -5.7243,\n",
      "           1.1378,  6.3828, -4.9347, -2.7538, -5.5704, -1.8131, -0.1127,\n",
      "          -3.3689,  2.6234]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "src_emb = src_embedding_block(toy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da373a07-8a19-4c70-b8d2-fdf22ca05680",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 16\n",
    "max_seq_len = 20\n",
    "\n",
    "src_embedding_block = EmbeddingBlock(tokenizer.vocab_size, embed_dim, max_seq_len, pad_id=tokenizer.pad_token_id)\n",
    "tgt_embedding_block = EmbeddingBlock(tokenizer.vocab_size, embed_dim, max_seq_len, pad_id=tokenizer.pad_token_id)\n",
    "\n",
    "for batch_idx, (x_batch, y_batch) in enumerate(dataloader[:5]):\n",
    "    src_emb = src_embedding_block(x_batch)\n",
    "    tgt_emb = tgt_embedding_block(y_batch)\n",
    "    print(src_emb.shape, tgt_emb.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b222c4fe-7e67-48f5-ac11-46aca6c64f6c",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a5b0648-d566-4dc5-b4c5-349c02a0fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = generate_random_dates()\n",
    "df = pd.DataFrame(dataset, columns=[\"x\", \"y\"])\n",
    "\n",
    "date_dataset = DateDataset(dataset, tokenizer)\n",
    "dataloader = DataLoader(date_dataset, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0aa71756-d219-4db0-a0eb-6b8fd7051bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 12, 16]) torch.Size([5, 20, 16])\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 16\n",
    "max_seq_len = 20\n",
    "\n",
    "src_embedding_block = EmbeddingBlock(tokenizer.vocab_size, embed_dim, max_seq_len, pad_id=tokenizer.pad_token_id)\n",
    "tgt_embedding_block = EmbeddingBlock(tokenizer.vocab_size, embed_dim, max_seq_len, pad_id=tokenizer.pad_token_id)\n",
    "\n",
    "for batch_idx, (x_batch, y_batch) in enumerate(dataloader):\n",
    "    src_emb = src_embedding_block(x_batch)\n",
    "    tgt_emb = tgt_embedding_block(y_batch)\n",
    "    print(src_emb.shape, tgt_emb.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a7d56ebe-a939-4af5-af47-73fb0dfd171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x[0])\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        print(Q[0])\n",
    "\n",
    "        scores = Q @ K.transpose(-2, -1) # QK^T / sqrt(d_k)\n",
    "\n",
    "        print(scores[0])\n",
    "        scores = F.softmax(Q @ K.transpose(-2, -1) / (self.embed_dim ** 0.5)) # QK^T / sqrt(d_k)\n",
    "        print(scores[0])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b51c50b1-f391-41c4-afd3-5eab2d173ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.2186e-01, -3.6626e+00,  3.5067e+00,  0.0000e+00, -4.5138e+00,\n",
      "          9.5425e+00,  4.1403e-01,  2.3382e+00,  8.5562e+00,  0.0000e+00,\n",
      "         -0.0000e+00,  5.2408e+00,  0.0000e+00,  4.0205e+00, -5.1358e+00,\n",
      "          3.5088e-01],\n",
      "        [-2.9718e+00,  6.6420e+00, -2.3001e+00, -1.5172e+00,  3.7740e+00,\n",
      "          1.8741e+00, -1.7106e+00, -5.2552e+00, -0.0000e+00,  1.5264e+00,\n",
      "          1.6031e+00,  1.6955e+00, -4.1848e+00, -5.1916e+00,  8.9527e+00,\n",
      "          7.7908e+00],\n",
      "        [-3.4834e+00, -2.5078e+00,  1.8389e+00,  1.3833e+00,  2.9731e+00,\n",
      "          0.0000e+00, -7.0196e+00, -5.3497e-01, -0.0000e+00,  1.2361e-01,\n",
      "          3.9371e+00,  4.2703e-01, -0.0000e+00,  1.8078e-01,  0.0000e+00,\n",
      "         -1.5949e+00],\n",
      "        [ 3.4584e+00, -2.7426e+00,  4.8643e+00,  1.5634e+00,  5.3393e+00,\n",
      "         -1.9466e+00, -5.1369e+00,  4.0558e+00,  1.2547e+00, -5.4791e-01,\n",
      "          1.9442e+00, -4.4357e+00, -1.2278e+00, -3.5357e+00, -1.0690e+00,\n",
      "         -1.8907e+00],\n",
      "        [-5.3346e+00, -2.7716e+00,  2.2416e+00,  8.2169e-01,  3.1850e+00,\n",
      "         -6.4272e-03, -6.9496e+00, -5.4162e-01, -0.0000e+00,  1.2295e-01,\n",
      "          3.9441e+00,  4.2697e-01, -2.8048e+00,  1.8077e-01,  2.6623e+00,\n",
      "         -1.5949e+00],\n",
      "        [ 4.2160e+00,  6.0999e+00,  5.0990e+00,  7.8818e+00,  2.5521e+00,\n",
      "          8.2088e+00, -1.1414e-03, -5.4897e+00,  3.1145e+00, -0.0000e+00,\n",
      "         -5.6621e+00, -1.9600e+00, -5.4282e-01,  2.1658e+00,  2.3610e+00,\n",
      "         -3.1354e+00],\n",
      "        [-0.0000e+00, -3.7390e+00, -4.8792e+00, -1.8112e+00,  5.8357e+00,\n",
      "          7.5920e+00, -1.6223e+00, -3.5713e+00, -4.4001e+00,  2.1281e+00,\n",
      "         -2.5845e+00,  2.8982e+00,  2.0562e+00,  3.2655e+00, -0.0000e+00,\n",
      "         -2.0336e+00],\n",
      "        [-3.7637e+00, -1.2077e+00,  2.0714e+00, -1.7895e-01,  3.4682e+00,\n",
      "         -1.8000e-01, -6.8458e+00, -5.5986e-01, -6.6828e+00,  1.2112e-01,\n",
      "          3.9546e+00,  4.2678e-01, -2.8015e+00,  1.8075e-01,  2.6634e+00,\n",
      "         -1.5949e+00],\n",
      "        [ 6.3808e+00,  5.6231e+00,  4.6261e+00,  6.9837e+00,  2.8165e+00,\n",
      "          8.0078e+00,  1.0201e-01, -5.5112e+00,  3.1477e+00, -1.6753e-01,\n",
      "         -5.6516e+00, -1.9602e+00, -5.3948e-01,  2.1658e+00,  2.3620e+00,\n",
      "         -3.1354e+00],\n",
      "        [-3.4489e+00,  5.0293e+00, -2.3220e+00, -3.6362e+00,  4.5335e+00,\n",
      "          1.4592e+00, -1.4338e+00, -5.2993e+00, -2.9296e+00,  1.5219e+00,\n",
      "          1.6312e+00,  1.6950e+00, -4.1759e+00, -0.0000e+00,  8.9555e+00,\n",
      "          7.7908e+00],\n",
      "        [-2.1122e+00, -5.7381e+00, -5.9546e+00, -2.5656e+00,  6.1433e+00,\n",
      "          7.2753e+00, -1.4863e+00, -3.6065e+00, -4.3558e+00,  2.1245e+00,\n",
      "         -0.0000e+00,  2.8979e+00,  2.0607e+00,  3.2655e+00, -0.0000e+00,\n",
      "         -2.0336e+00],\n",
      "        [-5.3601e+00,  2.7651e+00, -1.3720e-02, -0.0000e+00, -5.6076e-01,\n",
      "         -4.8496e+00,  4.5023e+00, -1.9643e+00, -3.6868e+00,  1.1377e+00,\n",
      "         -7.0028e-01,  6.6623e+00, -3.3858e+00,  9.9814e+00, -5.0630e+00,\n",
      "          2.0673e+00]], grad_fn=<SelectBackward0>)\n",
      "tensor([[-5.3722,  2.2637, -0.5224, -0.3861,  0.4298,  1.8936,  3.9763, -2.7155,\n",
      "         -0.4836,  2.1868, -3.9703,  1.8285,  4.5736, -1.4400, -4.0724, -2.2881],\n",
      "        [ 2.5333, -0.7680, -4.5490, -2.4515, -1.6429, -1.8835, -3.0395, -0.1940,\n",
      "          1.0119, -0.8535,  1.2973, -3.9112, -0.3497, -0.5055, -3.5764,  2.3343],\n",
      "        [-0.6633, -1.7323,  0.3803,  1.2296,  0.1257,  0.6558,  1.8196, -1.8033,\n",
      "          0.9513, -0.1681, -1.4322, -1.4201,  2.6030,  1.2728,  0.5597,  0.0581],\n",
      "        [-1.1281,  0.1352,  2.4843,  3.0821,  0.6143,  0.6082,  1.5134,  0.6968,\n",
      "          1.9701,  0.0772, -0.0232,  1.1438,  1.1818, -0.7483,  4.5186, -0.7541],\n",
      "        [-0.2326, -1.0803, -0.9521,  1.6124,  1.9409,  0.8966,  1.7108, -3.1173,\n",
      "          0.7319, -0.7938, -1.0778, -1.6882,  3.2620,  1.4377, -0.4939,  0.8454],\n",
      "        [-1.1619, -1.5551, -2.8404, -3.3416, -3.4627,  0.4306,  1.9667,  4.7206,\n",
      "          3.4393, -3.3045, -3.9050, -1.3515,  3.5968,  0.6517,  1.3403,  3.1631],\n",
      "        [ 0.8511, -2.8808,  1.4475, -4.4968, -3.1948,  1.8562,  3.1240,  2.2707,\n",
      "          0.5873, -0.7140, -1.5326, -3.8313,  3.0019,  0.3288, -0.3203,  2.9282],\n",
      "        [ 1.4814, -2.6192, -1.1743,  1.2880,  0.4678, -0.9487,  1.6352, -1.7891,\n",
      "          1.7313, -2.0288, -0.4920, -1.6270,  1.7345,  0.4280,  1.0947,  1.5576],\n",
      "        [-0.8499, -1.5597, -2.0861, -3.5400, -3.7733,  0.2082,  1.3206,  5.0828,\n",
      "          3.6766, -3.3331, -3.8041, -1.0875,  3.0915,  0.3418,  1.6727,  2.5287],\n",
      "        [ 3.7685, -1.7350, -5.3563, -2.6819, -0.9046, -2.0674, -2.4176, -0.1212,\n",
      "          0.5315, -2.1604,  1.3495, -4.8027, -0.9994,  0.3880, -3.8890,  2.8605],\n",
      "        [ 0.8674, -3.0648,  2.0719, -4.0062, -2.6058,  2.3527,  3.0561,  1.3587,\n",
      "         -0.3638, -0.0533, -0.9459, -4.0529,  3.1687,  0.4380, -1.2941,  2.6970],\n",
      "        [ 0.9233, -0.7107, -4.1808,  1.2652,  0.1771, -2.2447,  1.5565, -1.3245,\n",
      "         -0.1077, -2.7958,  1.0062, -0.5034, -1.4994,  2.4638, -1.1170,  2.8477]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[-28.3141,  50.9765,  -2.0059,  -3.9184,   4.7678,  16.4139, -22.3325,\n",
      "          25.5244,  14.6327,  42.7111, -25.7873, -33.4887],\n",
      "        [ -6.6772, -28.1211,   9.2665,  19.7361,   3.9109,  12.0353,  -0.7574,\n",
      "           0.2899,  10.4305, -34.0858,  -1.3091,  -5.3410],\n",
      "        [  0.6178,  23.0422,   0.8847,  -4.9959,   6.1892,  16.4145,   3.2645,\n",
      "          12.3138,  15.2171,  17.5154,   1.9616,  -3.1963],\n",
      "        [ 10.2302,   7.2904, -11.5278, -24.8697,  -6.3801,  -2.2071,   4.4596,\n",
      "          -4.4558,  -0.9822,  14.0340,   5.8284,  24.0612],\n",
      "        [  5.7647,  24.5462,   4.2072,   7.1719,   8.4603,  14.8033, -14.4660,\n",
      "          12.1912,  12.7072,  10.5479, -14.0635, -13.2978],\n",
      "        [-18.6631, -13.7709, -17.2785, -25.5467, -17.4306,  -1.7769,  -1.3875,\n",
      "          -4.3559,   0.1109, -10.3509,  -4.5136,  15.9225],\n",
      "        [-27.5427, -12.0567,  -8.5711, -15.4876, -11.8530,  17.7121,  52.0695,\n",
      "           6.6764,  22.0782,  -5.5744,  45.1629,  -0.6909],\n",
      "        [  0.9540,   9.7519,  -1.8516,  -3.3690,   0.5722,   9.0684,  -8.2218,\n",
      "           1.5725,   7.6905,   0.8712,  -8.4569,   4.9560],\n",
      "        [-19.9416, -13.0741, -17.6430, -31.0506, -17.5990,  -3.5815,   4.3679,\n",
      "          -5.1100,  -1.6723,  -6.2212,   1.2961,  21.5847],\n",
      "        [ -6.7035, -26.4750,   9.2932,  26.5290,   3.3696,   7.4828, -11.2861,\n",
      "          -4.4912,   5.2294, -38.9000, -12.0335,  -6.9633],\n",
      "        [-28.1367,  -5.3116,  -3.8087,  -9.6490,  -7.1986,  21.4162,  59.3882,\n",
      "          12.7909,  25.6616,  -0.4136,  52.6286, -10.0601],\n",
      "        [ 14.1463, -20.7733,   1.1108,  28.1916,  -3.4600, -12.2413, -48.5647,\n",
      "         -14.9268, -13.5121, -37.5001, -46.8791,  -9.1001]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[7.4965e-01, 5.2902e-01, 8.7573e-04, 9.9148e-03, 9.9713e-01, 7.8497e-01,\n",
      "         4.5584e-01, 9.9024e-01, 1.9874e-02, 9.7274e-01, 2.0973e-03, 1.6380e-03],\n",
      "        [5.3712e-03, 1.2291e-04, 2.4650e-01, 7.7727e-01, 2.5316e-01, 1.1695e-01,\n",
      "         1.2931e-01, 1.1883e-03, 3.3393e-01, 4.1126e-06, 2.3491e-01, 3.2017e-01],\n",
      "        [1.7694e-04, 8.9273e-01, 1.1920e-06, 2.5320e-02, 9.5937e-06, 5.6352e-03,\n",
      "         5.7013e-06, 2.5222e-01, 3.8358e-02, 2.2857e-03, 4.9076e-03, 6.6536e-02],\n",
      "        [1.2202e-02, 2.7732e-02, 1.0556e-04, 4.7296e-05, 7.3132e-04, 1.8753e-03,\n",
      "         1.0513e-04, 2.3036e-04, 1.2255e-03, 6.1151e-02, 2.7653e-03, 9.9779e-01],\n",
      "        [9.8376e-01, 9.8671e-01, 7.2010e-06, 1.9804e-06, 4.3425e-05, 1.0024e-02,\n",
      "         1.3860e-06, 3.1083e-01, 3.7418e-02, 9.0450e-01, 4.8834e-03, 1.3930e-04],\n",
      "        [7.1466e-04, 3.3264e-03, 2.6092e-03, 1.8436e-03, 2.5322e-03, 1.1273e-01,\n",
      "         4.0178e-03, 1.8316e-01, 4.3895e-01, 2.3895e-03, 2.1304e-01, 2.4493e-01],\n",
      "        [5.2790e-04, 4.5820e-03, 6.9699e-07, 4.4173e-02, 3.2359e-07, 1.2697e-02,\n",
      "         5.5276e-01, 2.6649e-01, 7.4454e-02, 4.9033e-06, 9.9973e-01, 6.2226e-01],\n",
      "        [3.9914e-01, 1.8805e-01, 1.9586e-05, 2.4927e-02, 2.4169e-01, 7.3570e-01,\n",
      "         2.8243e-03, 3.6209e-01, 6.4146e-01, 3.8075e-01, 4.0216e-01, 7.4991e-04],\n",
      "        [5.7986e-03, 4.5618e-03, 3.4914e-03, 1.2056e-03, 7.3954e-03, 5.8319e-03,\n",
      "         2.0661e-02, 4.5015e-01, 3.2025e-01, 5.3093e-03, 4.8439e-01, 3.9587e-01],\n",
      "        [1.1377e-03, 1.0959e-04, 6.7047e-01, 9.9786e-01, 9.9587e-03, 1.2694e-03,\n",
      "         9.6818e-08, 5.0751e-04, 1.3551e-02, 4.0193e-10, 2.1548e-03, 3.2118e-01],\n",
      "        [7.9553e-04, 8.6162e-03, 1.4968e-02, 4.0250e-02, 3.1294e-03, 8.7189e-01,\n",
      "         1.0000e+00, 4.1888e-01, 9.7302e-01, 1.6723e-01, 1.0000e+00, 3.7429e-05],\n",
      "        [2.8741e-01, 8.9032e-03, 6.0526e-04, 4.7426e-01, 3.8062e-01, 2.2249e-02,\n",
      "         1.3183e-02, 2.7046e-05, 6.6197e-02, 6.3814e-04, 1.8973e-05, 5.0080e-01]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46584/2923371407.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  scores = F.softmax(Q @ K.transpose(-2, -1) / (self.embed_dim ** 0.5)) # QK^T / sqrt(d_k)\n"
     ]
    }
   ],
   "source": [
    "att_layer = SelfAttention(embed_dim)\n",
    "\n",
    "x_batch, y_batch = next(iter(dataloader))\n",
    "src_emb = src_embedding_block(x_batch)\n",
    "tgt_emb = tgt_embedding_block(y_batch)\n",
    "att_layer.forward(src_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "35df3237-6d05-4cd1-9147-bf150dc8dc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46584/550296987.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  F.softmax(tensor / (16 ** 0.5))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.7853e-06, 5.8178e-08, 4.6816e-07, 3.9307e-04, 3.6159e-03, 3.8140e-09,\n",
       "         2.3492e-04, 9.9575e-01, 2.5387e-06, 3.1924e-07, 1.1114e-11, 5.8730e-06],\n",
       "        [5.0252e-04, 3.5492e-08, 6.8649e-08, 3.9705e-09, 3.7079e-10, 2.8930e-07,\n",
       "         1.0848e-09, 2.9016e-13, 6.7974e-09, 1.3509e-08, 9.9950e-01, 4.7645e-07],\n",
       "        [2.7718e-02, 3.1453e-07, 3.4799e-06, 4.9345e-07, 2.8376e-08, 4.2512e-07,\n",
       "         1.1130e-07, 2.6539e-11, 4.5982e-08, 2.3462e-07, 9.7227e-01, 4.9380e-06],\n",
       "        [1.0220e-02, 3.4668e-05, 5.2286e-04, 1.5500e-03, 2.1268e-02, 1.0303e-02,\n",
       "         7.3714e-03, 6.5058e-02, 8.9845e-03, 4.8506e-05, 6.8244e-01, 1.9220e-01],\n",
       "        [4.9741e-06, 2.7207e-06, 2.0228e-06, 8.2803e-07, 1.3623e-06, 1.0824e-04,\n",
       "         3.3796e-06, 2.5026e-08, 8.4744e-07, 2.0846e-06, 9.9983e-01, 4.2530e-05],\n",
       "        [3.5563e-02, 7.3574e-04, 1.1304e-03, 8.5620e-02, 1.5425e-01, 2.3884e-01,\n",
       "         4.3821e-03, 3.2166e-02, 4.4565e-01, 1.5964e-03, 4.1615e-05, 3.9023e-05],\n",
       "        [1.5693e-02, 2.4942e-06, 3.9531e-05, 3.3786e-05, 1.6298e-04, 4.5430e-03,\n",
       "         5.2183e-05, 8.6589e-05, 2.4368e-03, 2.4756e-06, 9.7503e-01, 1.9154e-03],\n",
       "        [2.8567e-04, 2.1631e-04, 4.1726e-05, 5.4243e-06, 9.3334e-07, 3.1512e-04,\n",
       "         2.7559e-06, 5.0740e-10, 1.2645e-05, 1.1974e-04, 9.9900e-01, 1.2946e-07],\n",
       "        [5.5818e-04, 9.5769e-06, 9.0905e-06, 6.4700e-03, 1.8776e-01, 3.9856e-03,\n",
       "         9.3660e-04, 7.3130e-01, 6.8923e-02, 3.3277e-05, 1.1822e-06, 1.3217e-05],\n",
       "        [3.5521e-05, 1.1431e-08, 1.3604e-08, 6.7244e-10, 4.7716e-11, 1.7692e-07,\n",
       "         3.3265e-10, 4.5200e-14, 2.5172e-09, 3.7436e-09, 9.9996e-01, 1.2908e-07],\n",
       "        [3.2994e-02, 9.0797e-03, 9.8566e-03, 1.1252e-02, 1.5091e-03, 8.5458e-01,\n",
       "         1.0493e-03, 5.4614e-05, 6.7786e-02, 6.1224e-03, 3.9628e-03, 1.7522e-03],\n",
       "        [3.6416e-01, 5.0149e-02, 4.7374e-01, 4.7708e-02, 6.4629e-04, 3.3109e-05,\n",
       "         3.9426e-03, 2.3250e-06, 3.9375e-04, 5.8855e-02, 3.7212e-04, 2.1331e-07]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[4.1520e-01, 9.2172e-03, 3.3761e-01, 7.7127e-02, 9.9264e-01, 2.0378e-03,\n",
    "         5.1287e-01, 9.8661e-01, 3.1220e-01, 7.5032e-07, 4.1305e-07, 1.4878e-01],\n",
    "        [1.1322e-02, 1.9390e-01, 1.9214e-05, 1.3463e-05, 1.7323e-09, 3.5755e-01,\n",
    "         1.8696e-02, 1.7061e-07, 1.6762e-02, 8.2477e-03, 9.9951e-01, 4.2487e-02],\n",
    "        [9.9716e-01, 7.5766e-03, 4.8501e-03, 9.0242e-03, 2.9205e-06, 1.6198e-04,\n",
    "         5.2688e-04, 1.2721e-07, 2.1545e-05, 1.4122e-03, 9.9999e-01, 6.3445e-04],\n",
    "        [1.9669e-01, 2.0403e-04, 4.9065e-03, 1.3530e-05, 1.1502e-02, 2.0921e-03,\n",
    "         2.9850e-02, 1.5834e-01, 9.3157e-05, 7.0591e-04, 9.7097e-01, 6.2658e-02],\n",
    "        [5.0417e-04, 4.0546e-01, 7.3268e-05, 1.4528e-03, 9.8513e-03, 1.7056e-01,\n",
    "         1.4156e-01, 2.2744e-03, 3.1547e-03, 2.1505e-04, 9.8825e-01, 2.0432e-01],\n",
    "        [5.1036e-02, 5.0841e-02, 5.5043e-04, 2.5777e-03, 2.4590e-02, 7.4514e-02,\n",
    "         1.1620e-01, 2.2663e-02, 9.1964e-02, 7.1103e-05, 2.3189e-06, 1.2055e-03],\n",
    "        [1.4029e-03, 7.0205e-03, 1.3617e-04, 9.4526e-04, 1.2528e-07, 5.4257e-01,\n",
    "         8.2813e-02, 2.3066e-01, 2.2557e-01, 2.0160e-03, 9.9531e-01, 1.5398e-01],\n",
    "        [2.0098e-03, 4.7694e-01, 2.1897e-03, 7.2547e-03, 1.0958e-05, 3.7231e-01,\n",
    "         6.5205e-06, 7.9622e-06, 1.7197e-02, 2.3918e-04, 9.9970e-01, 2.5197e-08],\n",
    "        [1.3795e-01, 2.2282e-02, 4.5764e-04, 7.5546e-03, 7.2052e-01, 4.9537e-02,\n",
    "         5.8559e-01, 9.8905e-01, 2.8492e-01, 1.1512e-04, 7.4283e-06, 3.4472e-02],\n",
    "        [9.8395e-03, 2.9270e-03, 1.7909e-04, 2.4486e-04, 7.7027e-10, 3.6916e-01,\n",
    "         8.4229e-03, 4.3075e-06, 1.9216e-02, 1.4292e-01, 9.9998e-01, 2.9361e-01],\n",
    "        [7.7648e-01, 1.2907e-02, 4.3021e-01, 9.4967e-02, 9.8623e-04, 5.8526e-01,\n",
    "         2.3055e-03, 4.6868e-07, 6.0341e-02, 1.2286e-02, 2.0148e-03, 6.7010e-05],\n",
    "        [2.1241e-02, 6.3361e-02, 5.5370e-01, 1.1581e-04, 1.7084e-03, 3.0007e-03,\n",
    "         2.6809e-02, 2.9881e-08, 5.1576e-02, 7.4330e-02, 3.5176e-04, 5.7024e-04]])\n",
    "\n",
    "tensor = torch.tensor([\n",
    "    [-5.3763, -19.0716, -10.7304, 16.2014, 25.0778, -29.9709, 14.1424, 47.5504, -3.9680, -12.2618, -53.3239, -0.6131],\n",
    "    [28.7324, -9.4999, -6.8611, -18.2616, -27.7456, -1.1073, -23.4516, -56.3575, -16.1110, -13.3637, 59.1139, 0.8883],\n",
    "    [33.2356, -12.3105, -2.6957, -10.5091, -21.9326, -11.1053, -16.4659, -49.8313, -20.0018, -13.4830, 47.4658, -1.2959],\n",
    "    [6.2911, -16.4541, -5.6001, -1.2533, 9.2225, 6.3236, 4.9841, 13.6948, 5.7757, -15.1106, 23.0964, 18.0278],\n",
    "    [0.9326, -1.4808, -2.6664, -6.2392, -4.2478, 13.2531, -0.6133, -20.2357, -6.1465, -2.5461, 49.7770, 9.5165],\n",
    "    [-2.5958, -18.1085, -16.3908, 0.9187, 3.2732, 5.0221, -10.9709, -2.9974, 7.5171, -15.0100, -29.5982, -29.8554],\n",
    "    [16.0027, -18.9854, -7.9330, -8.5611, -2.2667, 11.0441, -6.8223, -4.7966, 8.5525, -19.0153, 32.5196, 7.5894],\n",
    "    [11.6683, 10.5559, 3.9735, -4.1875, -11.2270, 12.0608, -6.8960, -41.2959, -0.8020, 8.1903, 44.3070, -19.1285],\n",
    "    [-1.0612, -17.3225, -17.5310, 8.7398, 22.2118, 6.8019, 1.0091, 27.6504, 18.2031, -12.3405, -25.6904, -16.0338],\n",
    "    [26.6676, -5.4987, -4.8026, -16.8313, -27.4139, 5.4588, -19.6466, -55.2616, -11.5514, -9.9637, 67.6490, 4.1979],\n",
    "    [5.6592, 0.4981, 0.8265, 1.3560, -6.6801, 18.6764, -8.1335, -19.9559, 8.5394, -1.0782, -2.8182, -6.0825],\n",
    "    [11.7751, 3.8447, 12.8273, 3.6451, -13.5613, -25.4471, -6.3279, -36.0715, -15.5435, 4.4850, -15.7695, -45.6263]\n",
    "])\n",
    "\n",
    "\n",
    "F.softmax(tensor / (16 ** 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eacb0e9-4c86-44a5-86bb-011a65426f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EncoderLayer(nn.Module):\n",
    "#     def __init__(self,embed_dim, num_heads, ff_hidden_dim, dropout=0.1):\n",
    "#         super.__init__()\n",
    "#         self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "#         self.norm1 = nn.LayerNorm(embed_dim)\n",
    "#         self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "#         self.ff = nn.Sequential(\n",
    "#             nn.Linear(embed_dim, ff_hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(ff_hidden_dim, embed_dim),\n",
    "#         )\n",
    "#         self.norm2 = nn.LayerNorm(embed_dim)\n",
    "#         self.dropout2 = nn.Dropout(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d110d8ca-f796-4c86-a63f-6d51ad06f1e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
